{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13131c34",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from string import punctuation\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9fb630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c789326",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741664e",
   "metadata": {},
   "source": [
    "# Vars definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram = 3\n",
    "window_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7705095",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c703932",
   "metadata": {},
   "source": [
    "## Initial clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28113d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30fefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = df[[\"review\"]]\n",
    "df_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_dataframe(\n",
    "    df_i: pd.DataFrame,\n",
    "    columns: list[str] | None = None,\n",
    "    keep_apostrophe: bool = True,\n",
    "    min_words: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Очищает текст и удаляет строки с малым количеством слов\n",
    "\n",
    "    Params:\n",
    "        df (pd.DataFrame): Исходный DataFrame\n",
    "        columns (list[str]|None): Столбцы для обработки (None = все строковые)\n",
    "        keep_apostrophe (bool): Сохранять апострофы (по умолчанию True)\n",
    "        min_words (int): Минимальное количество слов для сохранения строки\n",
    "\n",
    "    Return:\n",
    "        pd.DataFrame: Очищенная и отфильтрованная копия DataFrame\n",
    "    \"\"\"\n",
    "    df_clean = df_i.copy()\n",
    "\n",
    "    # Определение целевых столбцов\n",
    "    if columns is None:\n",
    "        columns = df_clean.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()\n",
    "\n",
    "    # Настройка паттерна для пунктуации\n",
    "    punct_pattern = r\"[{}]\".format(\n",
    "        re.escape(\n",
    "            punctuation.replace(\"'\", \"\") if keep_apostrophe else re.escape(punctuation)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    def text_cleaner(text):\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "\n",
    "        # Удаление эмодзи\n",
    "        text = emoji.replace_emoji(text, replace=\"\")\n",
    "\n",
    "        # Удаление пунктуации\n",
    "        text = re.sub(punct_pattern, \" \", text)\n",
    "\n",
    "        # Удаление спецсимволов\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\'\\s]\", \" \", text)\n",
    "\n",
    "        # Нормализация пробелов\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    for col in columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].apply(text_cleaner)\n",
    "\n",
    "    word_count_mask = (\n",
    "        df_clean[columns]\n",
    "        .apply(lambda col: col.str.split().str.len() > min_words)\n",
    "        .all(axis=1)\n",
    "    )\n",
    "\n",
    "    df_clean = df_clean[word_count_mask].reset_index(drop=True)\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ef03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = clean_text_dataframe(df_review, min_words=n_gram)\n",
    "df_review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bec623",
   "metadata": {},
   "source": [
    "## Data tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_dataframe(df_i: pd.DataFrame, tokenizer):\n",
    "    return pd.DataFrame(df_i.iloc[:, 0].apply(lambda col: tokenizer(col.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c35461",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens = tokenize_text_dataframe(\n",
    "    df_review, TweetTokenizer(match_phone_numbers=False).tokenize\n",
    ")\n",
    "df_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_text_dataframe(df_i: pd.DataFrame):\n",
    "    return pd.DataFrame(df_i.iloc[:, 0].apply(lambda col: sorted(set(col))))\n",
    "\n",
    "\n",
    "def idx_text_dataframe(df_i: pd.DataFrame):\n",
    "    return pd.DataFrame(\n",
    "        df_i.iloc[:, 0].apply(lambda col: {word: idx for idx, word in enumerate(col)})\n",
    "    )\n",
    "\n",
    "\n",
    "def global_idx_text_dataframe(df_i: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Создаёт словарь {слово: индекс} для всех уникальных слов\n",
    "    из объединённой первой колонки DataFrame, сохраняя порядок появления слов.\n",
    "    Возвращает DataFrame с одним словарём в виде строки.\n",
    "    \"\"\"\n",
    "    # Объединяем все элементы из первой колонки в один список\n",
    "    all_words = sum(df_i.iloc[:, 0].tolist(), [])\n",
    "\n",
    "    # Удаляем дубликаты с сохранением порядка первого появления\n",
    "    unique_words = list(set(all_words))\n",
    "\n",
    "    # Создаём итоговый словарь {слово: индекс}\n",
    "    combined_dict = {word: idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "    return pd.DataFrame(list(combined_dict.items()), columns=[\"Word\", \"Index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c641b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab = vocab_text_dataframe(df_tokens)\n",
    "df_word_to_idx = idx_text_dataframe(df_vocab)\n",
    "df_global_word_to_idx = global_idx_text_dataframe(df_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ccc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352df800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55395782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global_word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752712c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df_tokens.iloc[:, 0].to_list()\n",
    "vocab = df_vocab.iloc[:, 0].to_list()\n",
    "global_vocab = list(sorted(set([item for sublist in vocab for item in sublist])))\n",
    "vocab_size = len(global_vocab)\n",
    "word_to_idx = df_word_to_idx.iloc[:, 0].to_list()\n",
    "global_word_to_idx = df_global_word_to_idx.iloc[:, 0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b08711",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c31203",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2eddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57a931",
   "metadata": {},
   "source": [
    "## Token preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727457f6",
   "metadata": {},
   "source": [
    "### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e8f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, y_bow = [], []\n",
    "for idx, cur_token in enumerate(tokens\n",
    "                                [: 2 * len(tokens) // 3]\n",
    "                                ):\n",
    "    for i in range(len(cur_token) - window_size):\n",
    "        context = cur_token[i : i + window_size]\n",
    "        corpus.append(\" \".join(context))\n",
    "        y_bow.append(word_to_idx[idx][cur_token[i + window_size]])\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=global_vocab)\n",
    "X_bow = vectorizer.fit_transform(corpus).toarray()\n",
    "y_bow = np.array(y_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3bdfb1",
   "metadata": {},
   "source": [
    "#### df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98decfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bow = pd.DataFrame({\"x\": X_bow.tolist(), \"y\": y_bow.tolist()})\n",
    "# df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c9935",
   "metadata": {},
   "source": [
    "### N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_sequences = []\n",
    "for idx, cur_token in enumerate(tokens):\n",
    "    mass_sequences.append([])\n",
    "    for i in range(len(cur_token) - n_gram + 1):\n",
    "        mass_sequences[idx].append(cur_token[i : i + n_gram])\n",
    "\n",
    "X_ngram, y_ngram = [], []\n",
    "for idx, sequences in enumerate(mass_sequences):\n",
    "    for seq in sequences:\n",
    "        # print(seq, word_to_idx)\n",
    "        X_ngram.append([word_to_idx[idx][word] for word in seq[:-1]])\n",
    "        y_ngram.append(word_to_idx[idx][seq[-1]])\n",
    "\n",
    "X_ngram = np.array(X_ngram)\n",
    "y_ngram = np.array(y_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c2d22",
   "metadata": {},
   "source": [
    "#### df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ngram = pd.DataFrame({\"x\": X_ngram.tolist(), \"y\": y_ngram.tolist()})\n",
    "# df_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3a115",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(\n",
    "    X_bow, y_bow, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bfe3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ng, X_test_ng, y_train_ng, y_test_ng = train_test_split(\n",
    "    X_ngram, y_ngram, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc33782",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39041231",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a5e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history, title):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"Test Accuracy\")\n",
    "    plt.title(f\"{title} - Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Test Loss\")\n",
    "    plt.title(f\"{title} - Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a843a6a",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a1dd7",
   "metadata": {},
   "source": [
    "### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df21d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bow = Sequential(\n",
    "    [\n",
    "        Dense(128, activation=\"relu\", input_shape=(vocab_size,)),\n",
    "        Dense(vocab_size, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model_bow.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "bow_hist = model_bow.fit(\n",
    "    X_train_bow, y_train_bow, epochs=10, validation_data=(X_test_bow, y_test_bow)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491bd423",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(bow_hist, \"bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f201ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bow.save(\"models/model_bow.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23d874",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель RNN\n",
    "model_rnn = Sequential(\n",
    "    [\n",
    "        Embedding(vocab_size, 64, input_length=n_gram - 1),\n",
    "        SimpleRNN(128),\n",
    "        Dense(vocab_size, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model_rnn.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "rnn_hist = model_rnn.fit(\n",
    "    X_train_ng, y_train_ng, epochs=20, validation_data=(X_test_ng, y_test_ng)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7edb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(rnn_hist, \"rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.save(\"models/model_rnn.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab47413",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31869806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель GRU\n",
    "model_gru = Sequential(\n",
    "    [\n",
    "        Embedding(vocab_size, 64, input_length=n_gram - 1),\n",
    "        GRU(128),\n",
    "        Dense(vocab_size, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model_gru.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "gru_hist = model_gru.fit(\n",
    "    X_train_ng, y_train_ng, epochs=20, validation_data=(X_test_ng, y_test_ng)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f10c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(gru_hist, \"gru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeae90c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.save(\"models/model_gru.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27205343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка моделей\n",
    "def evaluate_model(model, X_test, y_test, name):\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    print(f\"\\n{name} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_bow, X_test_bow, y_test_bow, \"BoW\")\n",
    "evaluate_model(model_rnn, X_test_ng, y_test_ng, \"RNN\")\n",
    "evaluate_model(model_gru, X_test_ng, y_test_ng, \"GRU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e709943",
   "metadata": {},
   "source": [
    "# Word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e03023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(\n",
    "    model, input_sequence, word_to_idx, idx_to_word, mode=\"ngram\", top_k=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Предсказывает следующее слово на основе входной последовательности.\n",
    "\n",
    "    Параметры:\n",
    "        model: обученная модель (Keras или sklearn).\n",
    "        input_sequence: исходное предложение (строка).\n",
    "        word_to_idx: словарь для преобразования слов в индексы.\n",
    "        idx_to_word: словарь для преобразования индексов в слова.\n",
    "        mode: тип модели (\"ngram\" или \"bow\").\n",
    "        top_k: количество вариантов для вывода.\n",
    "    \"\"\"\n",
    "    # Токенизация и преобразование в нижний регистр\n",
    "    tokens = word_tokenize(input_sequence.lower())\n",
    "    tokens_idx = [\n",
    "        word_to_idx.get(word, -1) for word in tokens\n",
    "    ]  # -1 для неизвестных слов\n",
    "\n",
    "    # Обработка неизвестных слов (замена на <UNK> или пропуск)\n",
    "    tokens_idx = [\n",
    "        idx if idx != -1 else word_to_idx.get(\"<UNK>\", -1) for idx in tokens_idx\n",
    "    ]\n",
    "    if -1 in tokens_idx:\n",
    "        print(\"Есть неизвестные слова!\")\n",
    "        return []\n",
    "\n",
    "    # Подготовка данных в зависимости от типа модели\n",
    "    if mode == \"bow\":\n",
    "        # Используем последние window_size слов как контекст\n",
    "        window_size = 5  # Должно совпадать с обучением!\n",
    "        context = tokens_idx[-window_size:]\n",
    "        if len(context) < window_size:\n",
    "            # Дополняем нулями слева (pad_sequences)\n",
    "            context = [0] * (window_size - len(context)) + context\n",
    "\n",
    "        # Создаем вектор BoW (количество вхождений каждого слова)\n",
    "        bow_vector = np.zeros(len(word_to_idx))\n",
    "        for idx in context:\n",
    "            if idx < len(word_to_idx):\n",
    "                bow_vector[idx] += 1\n",
    "        input_data = bow_vector.reshape(1, -1)\n",
    "\n",
    "    elif mode == \"ngram\":\n",
    "        # Используем последние n-1 слов для N-граммной модели\n",
    "        n_gram = 3  # Должно совпадать с обучением!\n",
    "        seq_length = n_gram - 1\n",
    "        context = tokens_idx[-seq_length:]\n",
    "        if len(context) < seq_length:\n",
    "            # Дополняем нулями слева\n",
    "            context = [0] * (seq_length - len(context)) + context\n",
    "\n",
    "        input_data = np.array([context])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Режим должен быть 'bow' или 'ngram'\")\n",
    "\n",
    "    # Предсказание\n",
    "    preds = model.predict(input_data)[0]\n",
    "    top_indices = preds.argsort()[-top_k:][::-1]  # Топ-K индексов\n",
    "    top_words = [idx_to_word[idx] for idx in top_indices if idx in idx_to_word]\n",
    "\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61273fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"I love\"\n",
    "idx_to_word = {v: k for k, v in word_to_idx[0].items()}  # Создаем обратный словарь\n",
    "\n",
    "# Предсказание через BoW\n",
    "bow_prediction = predict_next_word(\n",
    "    model_bow, input_sentence, word_to_idx[0], idx_to_word, mode=\"bow\", top_k=3\n",
    ")\n",
    "\" \".join([input_sentence, bow_prediction[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa464bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"I love\"\n",
    "idx_to_word = {v: k for k, v in word_to_idx.items()}  # Создаем обратный словарь\n",
    "\n",
    "# Предсказание через BoW\n",
    "bow_prediction = predict_next_word(\n",
    "    model_rnn, input_sentence, word_to_idx, idx_to_word, mode=\"ngram\", top_k=3\n",
    ")\n",
    "\" \".join([input_sentence, bow_prediction[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2d67d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
