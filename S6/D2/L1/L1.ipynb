{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13131c34",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from string import punctuation\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9fb630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, SimpleRNN, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c789326",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741664e",
   "metadata": {},
   "source": [
    "# Vars definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram = 4\n",
    "window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7705095",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c703932",
   "metadata": {},
   "source": [
    "## Initial clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28113d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30fefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = df[[\"review\"]]\n",
    "df_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_dataframe(\n",
    "    df_i: pd.DataFrame,\n",
    "    columns: list[str] | None = None,\n",
    "    keep_apostrophe: bool = True,\n",
    "    min_words: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Очищает текст и удаляет строки с малым количеством слов\n",
    "\n",
    "    Params:\n",
    "        df (pd.DataFrame): Исходный DataFrame\n",
    "        columns (list[str]|None): Столбцы для обработки (None = все строковые)\n",
    "        keep_apostrophe (bool): Сохранять апострофы (по умолчанию True)\n",
    "        min_words (int): Минимальное количество слов для сохранения строки\n",
    "\n",
    "    Return:\n",
    "        pd.DataFrame: Очищенная и отфильтрованная копия DataFrame\n",
    "    \"\"\"\n",
    "    df_clean = df_i.copy()\n",
    "\n",
    "    # Определение целевых столбцов\n",
    "    if columns is None:\n",
    "        columns = df_clean.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()\n",
    "\n",
    "    # Настройка паттерна для пунктуации\n",
    "    punct_pattern = r\"[{}]\".format(\n",
    "        re.escape(\n",
    "            punctuation.replace(\"'\", \"\") if keep_apostrophe else re.escape(punctuation)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    def text_cleaner(text):\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "\n",
    "        # Удаление эмодзи\n",
    "        text = emoji.replace_emoji(text, replace=\"\")\n",
    "\n",
    "        # Удаление пунктуации\n",
    "        text = re.sub(punct_pattern, \" \", text)\n",
    "\n",
    "        # Удаление спецсимволов\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\'\\s]\", \" \", text)\n",
    "\n",
    "        # Нормализация пробелов\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    for col in columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].apply(text_cleaner)\n",
    "\n",
    "    word_count_mask = (\n",
    "        df_clean[columns]\n",
    "        .apply(lambda col: col.str.split().str.len() > min_words)\n",
    "        .all(axis=1)\n",
    "    )\n",
    "\n",
    "    df_clean = df_clean[word_count_mask].reset_index(drop=True)\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ef03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = clean_text_dataframe(df_review, min_words=n_gram)\n",
    "df_review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bec623",
   "metadata": {},
   "source": [
    "## Data tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_dataframe(df_i: pd.DataFrame, tokenizer):\n",
    "    return pd.DataFrame(\n",
    "        df_i.iloc[:, 0].apply(\n",
    "            lambda col: [\n",
    "                word for word in tokenizer(col.lower()) if word not in stop_words\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c35461",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens = tokenize_text_dataframe(\n",
    "    df_review, TweetTokenizer(match_phone_numbers=False).tokenize\n",
    ")\n",
    "df_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e939f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens = df_tokens[df_tokens[\"review\"].apply(lambda col: len(col) > n_gram)].reset_index(drop=True)\n",
    "df_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_text_dataframe(df_i: pd.DataFrame):\n",
    "    return pd.DataFrame(df_i.iloc[:, 0].apply(lambda col: sorted(set(col))))\n",
    "\n",
    "\n",
    "def idx_text_dataframe(df_i: pd.DataFrame):\n",
    "    return pd.DataFrame(\n",
    "        df_i.iloc[:, 0].apply(lambda col: {word: idx for idx, word in enumerate(col)})\n",
    "    )\n",
    "\n",
    "\n",
    "def global_idx_text_dataframe(df_i: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Создаёт словарь {слово: индекс} для всех уникальных слов\n",
    "    из объединённой первой колонки DataFrame, сохраняя порядок появления слов.\n",
    "    Возвращает DataFrame с одним словарём в виде строки.\n",
    "    \"\"\"\n",
    "    # Объединяем все элементы из первой колонки в один список\n",
    "    all_words = sum(df_i.iloc[:, 0].tolist(), [])\n",
    "\n",
    "    # Удаляем дубликаты с сохранением порядка первого появления\n",
    "    unique_words = list(set(all_words))\n",
    "\n",
    "    # Создаём итоговый словарь {слово: индекс}\n",
    "    combined_dict = {word: idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "    return pd.DataFrame(list(combined_dict.items()), columns=[\"Word\", \"Index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c641b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab = vocab_text_dataframe(df_tokens)\n",
    "df_word_to_idx = idx_text_dataframe(df_vocab)\n",
    "df_global_word_to_idx = global_idx_text_dataframe(df_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ccc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352df800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55395782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global_word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752712c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df_tokens.iloc[:, 0].to_list()\n",
    "vocab = df_vocab.iloc[:, 0].to_list()\n",
    "global_vocab = list(sorted(set([item for sublist in vocab for item in sublist])))\n",
    "vocab_size = len(global_vocab)\n",
    "word_to_idx = df_word_to_idx.iloc[:, 0].to_list()\n",
    "global_word_to_idx = {\n",
    "    v: k for k, v in df_global_word_to_idx.iloc[:, 0].to_dict().items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b08711",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c31203",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2eddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57a931",
   "metadata": {},
   "source": [
    "## Token preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727457f6",
   "metadata": {},
   "source": [
    "### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e8f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, y_bow = [], []\n",
    "for idx, cur_token in enumerate(tokens\n",
    "                                [: 2 * len(tokens) // 3]\n",
    "                                ):\n",
    "    for i in range(len(cur_token) - window_size):\n",
    "        context = cur_token[i : i + window_size]\n",
    "        corpus.append(\" \".join(context))\n",
    "        y_bow.append(word_to_idx[idx][cur_token[i + window_size]])\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=global_vocab)\n",
    "X_bow = vectorizer.fit_transform(corpus).toarray()\n",
    "y_bow = np.array(y_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3bdfb1",
   "metadata": {},
   "source": [
    "#### df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98decfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bow = pd.DataFrame({\"x\": X_bow.tolist(), \"y\": y_bow.tolist()})\n",
    "# df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c9935",
   "metadata": {},
   "source": [
    "### N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_sequences = []\n",
    "for idx, cur_token in enumerate(tokens):\n",
    "    mass_sequences.append([])\n",
    "    for i in range(len(cur_token) - n_gram + 1):\n",
    "        mass_sequences[idx].append(cur_token[i : i + n_gram])\n",
    "\n",
    "X_ngram, y_ngram = [], []\n",
    "for idx, sequences in enumerate(mass_sequences):\n",
    "    for seq in sequences:\n",
    "        # print(seq, word_to_idx)\n",
    "        X_ngram.append([word_to_idx[idx][word] for word in seq[:-1]])\n",
    "        y_ngram.append(word_to_idx[idx][seq[-1]])\n",
    "\n",
    "X_ngram = np.array(X_ngram)\n",
    "y_ngram = np.array(y_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c2d22",
   "metadata": {},
   "source": [
    "#### df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ngram = pd.DataFrame({\"x\": X_ngram.tolist(), \"y\": y_ngram.tolist()})\n",
    "# df_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3a115",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(\n",
    "    X_bow, y_bow, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bfe3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ng, X_test_ng, y_train_ng, y_test_ng = train_test_split(\n",
    "    X_ngram, y_ngram, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbfad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ng[0], y_train_ng[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a774c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ng.shape, y_train_ng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ng.shape, y_test_ng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a491ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow[0], y_train_bow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96155577",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow.shape, y_train_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3976dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bow.shape, y_test_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc33782",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39041231",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a5e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history, title):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"Test Accuracy\")\n",
    "    plt.title(f\"{title} - Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Test Loss\")\n",
    "    plt.title(f\"{title} - Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf8266",
   "metadata": {},
   "source": [
    "## Model preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef04d219",
   "metadata": {},
   "source": [
    "### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_model(m_type, shape):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Input((shape,)),\n",
    "            Dense(128, activation=\"relu\"),\n",
    "            Dense(vocab_size, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    if m_type == \"bow\":\n",
    "        c_loss = \"categorical_crossentropy\"\n",
    "    elif m_type == \"ng\":\n",
    "        c_loss = \"sparse_categorical_crossentropy\"\n",
    "    else:\n",
    "        print(\"WRONG TYPE!\")\n",
    "        return\n",
    "    model.compile(loss=c_loss, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17317d63",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(m_type, shape):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Embedding(vocab_size, 128, input_length=shape),\n",
    "            SimpleRNN(128),\n",
    "            Dense(vocab_size, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    if m_type == \"bow\":\n",
    "        c_loss = \"categorical_crossentropy\"\n",
    "    elif m_type == \"ng\":\n",
    "        c_loss = \"sparse_categorical_crossentropy\"\n",
    "    else:\n",
    "        print(\"WRONG TYPE!\")\n",
    "        return\n",
    "    model.compile(loss=c_loss, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2bd24f",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af61cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_model(m_type, shape):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Embedding(vocab_size, 128, input_length=shape),\n",
    "            GRU(128),\n",
    "            Dense(vocab_size, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    if m_type == \"bow\":\n",
    "        c_loss = \"categorical_crossentropy\"\n",
    "    elif m_type == \"ng\":\n",
    "        c_loss = \"sparse_categorical_crossentropy\"\n",
    "    else:\n",
    "        print(\"WRONG TYPE!\")\n",
    "        return\n",
    "    model.compile(loss=c_loss, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a4067",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88176eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(m_type, shape):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Embedding(vocab_size, 128, input_length=shape),\n",
    "            GRU(128),\n",
    "            Dense(vocab_size, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    if m_type == \"bow\":\n",
    "        c_loss = \"categorical_crossentropy\"\n",
    "    elif m_type == \"ng\":\n",
    "        c_loss = \"sparse_categorical_crossentropy\"\n",
    "    else:\n",
    "        print(\"WRONG TYPE!\")\n",
    "        return\n",
    "    model.compile(loss=c_loss, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a843a6a",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc93062",
   "metadata": {},
   "source": [
    "### Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44329c37",
   "metadata": {},
   "source": [
    "#### Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87538aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_ng = dense_model(\"ng\", n_gram - 1)\n",
    "dense_ng_hist = dense_ng.fit(X_train_ng, y_train_ng, epochs=20, validation_data=(X_test_ng, y_test_ng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a25ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(dense_ng_hist, \"dense_ng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_ng.save(\"models/dense/ng.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a1dd7",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df21d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_bow = dense_model(\"ng\", vocab_size)\n",
    "dense_bow_hist = dense_bow.fit(\n",
    "    X_train_bow, y_train_bow, epochs=20, validation_data=(X_test_bow, y_test_bow)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491bd423",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(dense_bow_hist, \"dense_bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f201ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_bow.save(\"models/dense/bow.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23d874",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29355349",
   "metadata": {},
   "source": [
    "#### Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bd4d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_ng = rnn_model(\"ng\", n_gram - 1)\n",
    "rnn_ng_hist = rnn_ng.fit(X_train_ng, y_train_ng, epochs=10, validation_data=(X_test_ng, y_test_ng))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(rnn_ng_hist, \"rnn_ng\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_ng.save(\"models/rnn/ng.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb26a2",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63121bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_bow = rnn_model(\"ng\", window_size)\n",
    "rnn_bow_hist = rnn_bow.fit(X_train_bow, y_train_bow, epochs=10, validation_data=(X_test_bow, y_test_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d4e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(rnn_bow_hist, \"rnn_bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b8557",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_bow.save(\"models/rnn/bow.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab47413",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cdbfad",
   "metadata": {},
   "source": [
    "#### Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c66887",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_ng = gru_model(\"ng\", n_gram - 1)\n",
    "gru_ng_hist = gru_ng.fit(X_train_ng, y_train_ng, epochs=20, validation_data=(X_test_ng, y_test_ng))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae7f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(gru_ng_hist, \"gru_ng\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54afcefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_ng.save(\"models/gru/ng.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621f4ae",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_bow = gru_model(\"ng\", window_size)\n",
    "gru_bow_hist = gru_bow.fit(X_train_bow, y_train_bow, epochs=10, validation_data=(X_test_bow, y_test_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26857685",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(gru_bow_hist, \"gru_bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_bow.save(\"models/gru/bow.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d51ad1",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67185f11",
   "metadata": {},
   "source": [
    "#### Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda6375",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_ng = lstm_model(\"ng\", n_gram - 1)\n",
    "lstm_ng_hist = lstm_ng.fit(X_train_bow, y_train_bow, epochs=10, validation_data=(X_test_bow, y_test_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab995a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(lstm_ng_hist, \"lstm_ng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d925c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_ng.save(\"models/lstm/ng.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ba09b",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64222a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_bow = lstm_model(\"ng\", window_size)\n",
    "lstm_bow_hist = lstm_bow.fit(X_train_bow, y_train_bow, epochs=10, validation_data=(X_test_bow, y_test_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93cb91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(lstm_bow_hist, \"lstm_bow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_bow.save(\"models/lstm/bow.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0522417",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27205343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, name):\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    print(f\"\\n{name} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    \n",
    "def eval_bow(model, name):\n",
    "    evaluate_model(model, X_test_bow, y_test_bow, name + \"_bow\")\n",
    "    \n",
    "def eval_ng(model, name):\n",
    "    evaluate_model(model, X_test_ng, y_test_ng, name + \"_ng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_bow(dense_bow, \"Dense\")\n",
    "eval_bow(rnn_bow, \"RNN\")\n",
    "eval_bow(gru_bow, \"GRU\")\n",
    "eval_bow(lstm_bow, \"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ng(dense_ng, \"Dense\")\n",
    "eval_ng(rnn_ng, \"RNN\")\n",
    "eval_ng(gru_ng, \"GRU\")\n",
    "eval_bow(lstm_ng, \"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e709943",
   "metadata": {},
   "source": [
    "# Word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e03023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(\n",
    "    model, input_sequence, word_to_idx, idx_to_word, mode=\"ngram\", top_k=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Предсказывает следующее слово на основе входной последовательности.\n",
    "\n",
    "    Параметры:\n",
    "        model: обученная модель (Keras или sklearn).\n",
    "        input_sequence: исходное предложение (строка).\n",
    "        word_to_idx: словарь для преобразования слов в индексы.\n",
    "        idx_to_word: словарь для преобразования индексов в слова.\n",
    "        mode: тип модели (\"ngram\" или \"bow\").\n",
    "        top_k: количество вариантов для вывода.\n",
    "    \"\"\"\n",
    "    # Токенизация и преобразование в нижний регистр\n",
    "    tokens = word_tokenize(input_sequence.lower())\n",
    "    tokens_idx = [\n",
    "        word_to_idx.get(word, -1) for word in tokens\n",
    "    ]  # -1 для неизвестных слов\n",
    "\n",
    "    # Обработка неизвестных слов (замена на <UNK> или пропуск)\n",
    "    tokens_idx = [\n",
    "        idx if idx != -1 else word_to_idx.get(\"<UNK>\", -1) for idx in tokens_idx\n",
    "    ]\n",
    "    if -1 in tokens_idx:\n",
    "        print(\"Есть неизвестные слова!\")\n",
    "        return []\n",
    "\n",
    "    # Подготовка данных в зависимости от типа модели\n",
    "    if mode == \"bow\":\n",
    "        # Используем последние window_size слов как контекст\n",
    "        # window_size = 5  # Должно совпадать с обучением!\n",
    "        context = tokens_idx[-window_size:]\n",
    "        if len(context) < window_size:\n",
    "            # Дополняем нулями слева (pad_sequences)\n",
    "            context = [0] * (window_size - len(context)) + context\n",
    "\n",
    "        # Создаем вектор BoW (количество вхождений каждого слова)\n",
    "        bow_vector = np.zeros(len(word_to_idx))\n",
    "        for idx in context:\n",
    "            if idx < len(word_to_idx):\n",
    "                bow_vector[idx] += 1\n",
    "        input_data = bow_vector.reshape(1, -1)\n",
    "\n",
    "    elif mode == \"ngram\":\n",
    "        # Используем последние n-1 слов для N-граммной модели\n",
    "        # n_gram = 3  # Должно совпадать с обучением!\n",
    "        seq_length = n_gram - 1\n",
    "        context = tokens_idx[-seq_length:]\n",
    "        if len(context) < seq_length:\n",
    "            # Дополняем нулями слева\n",
    "            context = [0] * (seq_length - len(context)) + context\n",
    "\n",
    "        input_data = np.array([context])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Режим должен быть 'bow' или 'ngram'\")\n",
    "\n",
    "    # Предсказание\n",
    "    preds = model.predict(input_data)[0]\n",
    "    top_indices = preds.argsort()[-top_k:][::-1]  # Топ-K индексов\n",
    "    top_words = [idx_to_word[idx] for idx in top_indices if idx in idx_to_word]\n",
    "\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model, \n",
    "    seed_text, \n",
    "    word_to_idx, \n",
    "    idx_to_word, \n",
    "    mode=\"ngram\", \n",
    "    num_words=5, \n",
    "    temperature=1.0, \n",
    "    top_k=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Генерирует последовательность слов на основе начального текста.\n",
    "    \n",
    "    Параметры:\n",
    "        model: обученная модель\n",
    "        seed_text: начальный текст (строка)\n",
    "        word_to_idx: словарь слово -> индекс\n",
    "        idx_to_word: словарь индекс -> слово\n",
    "        mode: \"ngram\" (RNN/GRU) или \"bow\"\n",
    "        num_words: количество слов для генерации\n",
    "        temperature: уровень случайности (0.1-2.0)\n",
    "        top_k: выбор из топ-K вероятных слов\n",
    "    \"\"\"\n",
    "    generated = seed_text.split()\n",
    "    tokens = word_tokenize(seed_text.lower())\n",
    "    tokens_idx = [word_to_idx.get(word, word_to_idx.get(\"<UNK>\", 0)) for word in tokens]\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        # Подготовка входных данных\n",
    "        if mode == \"bow\":\n",
    "            # window_size = 5\n",
    "            context = tokens_idx[-window_size:]\n",
    "            if len(context) < window_size:\n",
    "                context = [0] * (window_size - len(context)) + context\n",
    "            \n",
    "            bow_vector = np.zeros(len(word_to_idx))\n",
    "            for idx in context:\n",
    "                bow_vector[idx] += 1\n",
    "            input_data = bow_vector.reshape(1, -1)\n",
    "            \n",
    "        elif mode == \"ngram\":\n",
    "            # n_gram = 3\n",
    "            seq_length = n_gram - 1\n",
    "            context = tokens_idx[-seq_length:]\n",
    "            if len(context) < seq_length:\n",
    "                context = [0] * (seq_length - len(context)) + context\n",
    "            input_data = np.array([context])\n",
    "        \n",
    "        # Предсказание\n",
    "        preds = model.predict(input_data, verbose=0)[0]\n",
    "        preds = np.log(preds) / temperature  # Применяем температуру\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        \n",
    "        # Выбор из топ-K слов\n",
    "        top_indices = np.argpartition(preds, -top_k)[-top_k:]\n",
    "        top_probs = preds[top_indices]\n",
    "        top_probs = top_probs / np.sum(top_probs)  # Нормализуем\n",
    "        \n",
    "        # Сэмплирование\n",
    "        chosen_idx = np.random.choice(top_indices, p=top_probs)\n",
    "        chosen_word = idx_to_word.get(chosen_idx, \"<UNK>\")\n",
    "        \n",
    "        # Обновляем контекст\n",
    "        generated.append(chosen_word)\n",
    "        tokens_idx.append(chosen_idx)\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61273fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"love this model\"\n",
    "idx_to_word = {v: k for k, v in global_word_to_idx.items()}  # Создаем обратный словарь\n",
    "\n",
    "# Предсказание через BoW\n",
    "bow_prediction = generate_text(\n",
    "    dense_bow, input_sentence, global_word_to_idx, idx_to_word, mode=\"bow\", top_k=3\n",
    ")\n",
    "\" \".join(bow_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa464bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"love this model\"\n",
    "idx_to_word = {v: k for k, v in global_word_to_idx.items()}  # Создаем обратный словарь\n",
    "\n",
    "# Предсказание через BoW\n",
    "bow_prediction = generate_text(\n",
    "    rnn_ng, input_sentence, global_word_to_idx, idx_to_word, mode=\"ngram\", top_k=3\n",
    ")\n",
    "\" \".join(bow_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2d67d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
